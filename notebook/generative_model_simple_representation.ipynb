{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"e987866348dd4ae6b19d6b9ec8462b86":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d72f7fc9116249e2bc34192f2a59b23f","IPY_MODEL_5b5ebb1d77f04a479eb0356183a4f56c","IPY_MODEL_ffbe4c22a6b74326bf7d3be7af89f6be"],"layout":"IPY_MODEL_05b5291e29d74942b274757c8378915c"}},"d72f7fc9116249e2bc34192f2a59b23f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff25ab2452d5487d98770dcbca5da873","placeholder":"​","style":"IPY_MODEL_7343014c2e3f4dee940a64c91a0fc725","value":"config.json: 100%"}},"5b5ebb1d77f04a479eb0356183a4f56c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9eb8d9b3b1ab4875a2d4470e21939ae1","max":1527,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f2b8b0e7f9fb4269a75414b70749dd95","value":1527}},"ffbe4c22a6b74326bf7d3be7af89f6be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81af8caa984342cabce1faec4e83b0aa","placeholder":"​","style":"IPY_MODEL_531c852638794c1191ca41f555bb5291","value":" 1.53k/1.53k [00:00&lt;00:00, 34.4kB/s]"}},"05b5291e29d74942b274757c8378915c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff25ab2452d5487d98770dcbca5da873":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7343014c2e3f4dee940a64c91a0fc725":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9eb8d9b3b1ab4875a2d4470e21939ae1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2b8b0e7f9fb4269a75414b70749dd95":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"81af8caa984342cabce1faec4e83b0aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"531c852638794c1191ca41f555bb5291":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d01617462a884a84b68d4a80b678101b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c21ff5b2b04440f91489efd911cc006","IPY_MODEL_222ac169f9754571af697b7ab8d31dfc","IPY_MODEL_47779c5f2dd54b029f5df86e4f3c0899"],"layout":"IPY_MODEL_a237704019124fcf997eaf5345e76287"}},"2c21ff5b2b04440f91489efd911cc006":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b106a705c5947e6979fa842b6d0d20d","placeholder":"​","style":"IPY_MODEL_6ad71e1a8e59420c90bb967373667f6f","value":"pytorch_model.bin.index.json: 100%"}},"222ac169f9754571af697b7ab8d31dfc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cc2a79f6bcb4afbaad8ac1a3a787b91","max":50781,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fea2ef4441df485fbefefdfefbd509cd","value":50781}},"47779c5f2dd54b029f5df86e4f3c0899":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_677c7a5f81b44624bdff160117498908","placeholder":"​","style":"IPY_MODEL_1cad5a7dde144f09ae96dd1a301d2ef7","value":" 50.8k/50.8k [00:00&lt;00:00, 1.41MB/s]"}},"a237704019124fcf997eaf5345e76287":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b106a705c5947e6979fa842b6d0d20d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ad71e1a8e59420c90bb967373667f6f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1cc2a79f6bcb4afbaad8ac1a3a787b91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fea2ef4441df485fbefefdfefbd509cd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"677c7a5f81b44624bdff160117498908":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cad5a7dde144f09ae96dd1a301d2ef7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f78fbb2ed4745c399e098247955915a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a31ec43b4384eafaeacfbcdd2765802","IPY_MODEL_ba125a340d1d45228ad4b3a2b05bfeef","IPY_MODEL_907be164683d4bdf82713f2202ab3778"],"layout":"IPY_MODEL_e7139db8eb9f4c7da871630fbff077c1"}},"4a31ec43b4384eafaeacfbcdd2765802":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b4df8fc70be4f72aa961852981925b5","placeholder":"​","style":"IPY_MODEL_3d7a233dcffb499dac5eed85a7800afe","value":"Downloading shards: 100%"}},"ba125a340d1d45228ad4b3a2b05bfeef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc17d9bc458147a1b79685813b0dc0b3","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ba4e58f6f5d47b09878f8cda2865cc7","value":2}},"907be164683d4bdf82713f2202ab3778":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ce38ed74e52417bb99522f4fb29192d","placeholder":"​","style":"IPY_MODEL_6ef485ed0ebc4c15946efa77e1c33276","value":" 2/2 [04:23&lt;00:00, 116.94s/it]"}},"e7139db8eb9f4c7da871630fbff077c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b4df8fc70be4f72aa961852981925b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d7a233dcffb499dac5eed85a7800afe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc17d9bc458147a1b79685813b0dc0b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ba4e58f6f5d47b09878f8cda2865cc7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3ce38ed74e52417bb99522f4fb29192d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ef485ed0ebc4c15946efa77e1c33276":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01de4727549a4d24a453aa5e5cb79e3c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_98c925088cb34b2cbbb56fe51f036fc2","IPY_MODEL_fd38210fe5d44ad4a7a38b6f33afb698","IPY_MODEL_d58bc9b667b842f7966945f5adcf956a"],"layout":"IPY_MODEL_4a200973d7b643d9a4d5b5a432ff9d34"}},"98c925088cb34b2cbbb56fe51f036fc2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f6160c00cdc4379b6aa338807df22e6","placeholder":"​","style":"IPY_MODEL_c1156b8a7cc14329b8dff4e7f117e378","value":"pytorch_model-00001-of-00002.bin: 100%"}},"fd38210fe5d44ad4a7a38b6f33afb698":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c68c234998b40d188fab14a9fb09152","max":9449727999,"min":0,"orientation":"horizontal","style":"IPY_MODEL_309a010bbaf64e698ed3ec8724c6eb72","value":9449727999}},"d58bc9b667b842f7966945f5adcf956a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53012bae430f4b6199b0d9ca490c46f6","placeholder":"​","style":"IPY_MODEL_0813d8eb178a43dfac6de592e67fcba7","value":" 9.45G/9.45G [03:35&lt;00:00, 54.6MB/s]"}},"4a200973d7b643d9a4d5b5a432ff9d34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f6160c00cdc4379b6aa338807df22e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1156b8a7cc14329b8dff4e7f117e378":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c68c234998b40d188fab14a9fb09152":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"309a010bbaf64e698ed3ec8724c6eb72":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53012bae430f4b6199b0d9ca490c46f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0813d8eb178a43dfac6de592e67fcba7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc6fea9c48d04c60b2885c0459200922":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4353be6bd3814f27ae064bf0054e9937","IPY_MODEL_a8a18565b84e4b78bc85d26b274b9792","IPY_MODEL_b3342f9729dd432a9d7c5440da4efdd9"],"layout":"IPY_MODEL_c018d1f7ba794a5daaeb2ea76a43476a"}},"4353be6bd3814f27ae064bf0054e9937":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7625d9721be4e7ab812c125637ada4f","placeholder":"​","style":"IPY_MODEL_ba4a415bd2444911944d22d0a4e747bb","value":"pytorch_model-00002-of-00002.bin: 100%"}},"a8a18565b84e4b78bc85d26b274b9792":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef6ee643edf34124a0e25873ba798140","max":1949497635,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db9d5768041c4e9b91d5b263fddee6bd","value":1949497635}},"b3342f9729dd432a9d7c5440da4efdd9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5adff5a6176b481e9cedd981e8397d7f","placeholder":"​","style":"IPY_MODEL_46303148d73b438483ed1043b65dc500","value":" 1.95G/1.95G [00:46&lt;00:00, 31.1MB/s]"}},"c018d1f7ba794a5daaeb2ea76a43476a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7625d9721be4e7ab812c125637ada4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba4a415bd2444911944d22d0a4e747bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef6ee643edf34124a0e25873ba798140":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db9d5768041c4e9b91d5b263fddee6bd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5adff5a6176b481e9cedd981e8397d7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46303148d73b438483ed1043b65dc500":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"921580e11cdf477baf2a05f1e7a3e8bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92812138c1094a689b234fab11fb9c5e","IPY_MODEL_5db2f5e418674d2fb2f43dd2dec76377","IPY_MODEL_3d3fef0e7cd2474197e48e7b8f346409"],"layout":"IPY_MODEL_486dc25aa1e14560906d105fed6555bb"}},"92812138c1094a689b234fab11fb9c5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86ef9b185d7940419efd818d424c3f7c","placeholder":"​","style":"IPY_MODEL_f599b996707645e9804ca6a4a4ae20c7","value":"Loading checkpoint shards:   0%"}},"5db2f5e418674d2fb2f43dd2dec76377":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ac7a7b8d5b042129000e1f86a19efd2","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d7987db8141e493abee62ea5cd706e28","value":0}},"3d3fef0e7cd2474197e48e7b8f346409":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1774737ea30b43cdb62b955aed1729c0","placeholder":"​","style":"IPY_MODEL_6479338d18f949d99ea4b09c236ee8c0","value":" 0/2 [00:00&lt;?, ?it/s]"}},"486dc25aa1e14560906d105fed6555bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86ef9b185d7940419efd818d424c3f7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f599b996707645e9804ca6a4a4ae20c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ac7a7b8d5b042129000e1f86a19efd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7987db8141e493abee62ea5cd706e28":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1774737ea30b43cdb62b955aed1729c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6479338d18f949d99ea4b09c236ee8c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["## Installation and Setup"],"metadata":{"id":"GrLWLle74-mC"}},{"cell_type":"code","source":["import networkx as nx\n","!pip install transformers\n","!pip install sentencepiece\n","from transformers import pipeline"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2IVQbA_GeDqN","outputId":"8d773b1e-fdfd-434a-d929-f7683ef0193c","executionInfo":{"status":"ok","timestamp":1703119386222,"user_tz":360,"elapsed":34262,"user":{"displayName":"Vedant Tewari","userId":"00461717556933698773"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n"]}]},{"cell_type":"markdown","source":["### Using flan alpaca model for speed and local execution."],"metadata":{"id":"ppJQOfF-3HT1"}},{"cell_type":"code","source":["prompt = \"Write an email about an alpaca that likes flan\"\n","model = pipeline(model=\"declare-lab/flan-alpaca-xl\", device=0)\n","model(prompt, max_length=128, do_sample=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209,"referenced_widgets":["e987866348dd4ae6b19d6b9ec8462b86","d72f7fc9116249e2bc34192f2a59b23f","5b5ebb1d77f04a479eb0356183a4f56c","ffbe4c22a6b74326bf7d3be7af89f6be","05b5291e29d74942b274757c8378915c","ff25ab2452d5487d98770dcbca5da873","7343014c2e3f4dee940a64c91a0fc725","9eb8d9b3b1ab4875a2d4470e21939ae1","f2b8b0e7f9fb4269a75414b70749dd95","81af8caa984342cabce1faec4e83b0aa","531c852638794c1191ca41f555bb5291","d01617462a884a84b68d4a80b678101b","2c21ff5b2b04440f91489efd911cc006","222ac169f9754571af697b7ab8d31dfc","47779c5f2dd54b029f5df86e4f3c0899","a237704019124fcf997eaf5345e76287","2b106a705c5947e6979fa842b6d0d20d","6ad71e1a8e59420c90bb967373667f6f","1cc2a79f6bcb4afbaad8ac1a3a787b91","fea2ef4441df485fbefefdfefbd509cd","677c7a5f81b44624bdff160117498908","1cad5a7dde144f09ae96dd1a301d2ef7","9f78fbb2ed4745c399e098247955915a","4a31ec43b4384eafaeacfbcdd2765802","ba125a340d1d45228ad4b3a2b05bfeef","907be164683d4bdf82713f2202ab3778","e7139db8eb9f4c7da871630fbff077c1","3b4df8fc70be4f72aa961852981925b5","3d7a233dcffb499dac5eed85a7800afe","bc17d9bc458147a1b79685813b0dc0b3","4ba4e58f6f5d47b09878f8cda2865cc7","3ce38ed74e52417bb99522f4fb29192d","6ef485ed0ebc4c15946efa77e1c33276","01de4727549a4d24a453aa5e5cb79e3c","98c925088cb34b2cbbb56fe51f036fc2","fd38210fe5d44ad4a7a38b6f33afb698","d58bc9b667b842f7966945f5adcf956a","4a200973d7b643d9a4d5b5a432ff9d34","4f6160c00cdc4379b6aa338807df22e6","c1156b8a7cc14329b8dff4e7f117e378","2c68c234998b40d188fab14a9fb09152","309a010bbaf64e698ed3ec8724c6eb72","53012bae430f4b6199b0d9ca490c46f6","0813d8eb178a43dfac6de592e67fcba7","fc6fea9c48d04c60b2885c0459200922","4353be6bd3814f27ae064bf0054e9937","a8a18565b84e4b78bc85d26b274b9792","b3342f9729dd432a9d7c5440da4efdd9","c018d1f7ba794a5daaeb2ea76a43476a","a7625d9721be4e7ab812c125637ada4f","ba4a415bd2444911944d22d0a4e747bb","ef6ee643edf34124a0e25873ba798140","db9d5768041c4e9b91d5b263fddee6bd","5adff5a6176b481e9cedd981e8397d7f","46303148d73b438483ed1043b65dc500","921580e11cdf477baf2a05f1e7a3e8bf","92812138c1094a689b234fab11fb9c5e","5db2f5e418674d2fb2f43dd2dec76377","3d3fef0e7cd2474197e48e7b8f346409","486dc25aa1e14560906d105fed6555bb","86ef9b185d7940419efd818d424c3f7c","f599b996707645e9804ca6a4a4ae20c7","0ac7a7b8d5b042129000e1f86a19efd2","d7987db8141e493abee62ea5cd706e28","1774737ea30b43cdb62b955aed1729c0","6479338d18f949d99ea4b09c236ee8c0"]},"id":"_5tljpoz0PNA","outputId":"6a825170-950b-469a-e36c-b5f770d22cf7"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e987866348dd4ae6b19d6b9ec8462b86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin.index.json:   0%|          | 0.00/50.8k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d01617462a884a84b68d4a80b678101b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f78fbb2ed4745c399e098247955915a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01de4727549a4d24a453aa5e5cb79e3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model-00002-of-00002.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc6fea9c48d04c60b2885c0459200922"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"921580e11cdf477baf2a05f1e7a3e8bf"}},"metadata":{}}]},{"cell_type":"code","source":["prompt = '''### Instruction:\n","Who is the first person on the moon?\n","\n","### Response:'''\n","def generate(prompt):\n","  output = model(prompt, do_sample=True, min_length=10, max_length=len(prompt)+128)\n","  out = output[0]['generated_text']\n","  if '### Response:' in out:\n","    out = out.split('### Response:')[1]\n","  if '### Instruction:' in out:\n","    out = out.split('### Instruction:')[0]\n","  return out"],"metadata":{"id":"M10a7xJomV-B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## World Description\n","Below, is the detail the of the world for reference. From this information, prompts are generated. The simulated setting is the town of Phandalin, situated southwest of Neverwinter. The area was selected for its scalability, offering multiple regions for players to explore once the simulation is complete."],"metadata":{"id":"4TjpUGKT42o_"}},{"cell_type":"code","source":["# Creating a graph to represent the world and its connections\n","world_graph = nx.Graph()\n","\n","# Defining the prompt structure\n","prompt_meta = '''### Instruction:\n","{}\n","\n","### Response:'''\n","\n","# List of town areas and their descriptions\n","town_areas = [\"Barthen's Provisions\", \"Lionshield Coster\", \"Stonehill Inn\", \"Phandalin Town Square\"]\n","town_areas = {\"Phandalin Town Square\": 'Town square of the town of Phandalin.',\n","              'Stonehill Inn': \"In the center of town stands a large, newly built roadhouse of fieldstone and rough-hewn timbers. The common room is filled with locals nursing mugs of ale or cider, all of them eyeing you with curiosity.\",\n","              \"Barthen's Provisions\": \"Barthen’s is the biggest trading post in Phandalin. Its shelves stock most ordinary goods and supplies, including backpacks, bedrolls, rope, and rations. The place is open from sunup to sundown.\",\n","              \"Edermath Orchard\": \"A tidy little cottage beside an apple orchard.\",\n","              \"Lionshield Coster\": \"Hanging above the front door of this modest trading post is a sign shaped like a wooden shield with a blue lion painted on it. This building is owned by the Lionshields, a merchant company based in the city of Yartar, over a hundred miles to the east. They ship finished goods to Phandalin and other small settlements throughout the region, but this outpost has been hard hit by banditry. The most recent Lionshield caravan due in Phandalin never arrived.\",\n","              \"Phandalin Miner's Exchange\": \"The Miner’s Exchange is a trading post where local miners have their valuable finds weighed, measured, and paid out. In the absence of any local lord or authority, the exchange also serves as an unofficial records office, registering claims to various streams and excavations around the area. There isn’t any real gold rush in Phandalin, but enough wealth is hidden in the nearby streams and valleys to support a good number of independent prospectors. The exchange is a great place to meet people who spend a lot of time out and about in the countryside surrounding Phandalin. The guildmaster is an ambitious and calculating human woman named Halia Thornton.\",\n","              \"Alderleaf Farm\": \"A farm owned by the helpful halfling farmer, Qelline Alderleaf.\",\n","              \"Shrine of Luck\": \"Phandalin's only temple is a small shrine made of stones taken from the nearby ruins. It is dedicated to Tymora, goddess of luck and good fortune.\",\n","              \"The Sleeping Giant\": \"This rundown tap house is a dirty, dangerous watering hole at the end of Phandalin’s main street. It is frequented by Redbrand thugs and operated by a surly female dwarf named Grista.\",\n","              \"Townmaster’s Hall\": \"The townmaster’s hall has sturdy stone walls, a pitched wooden roof, and a bell tower at the back. Posted on a board next to the front door is a notice written in Common. It reads: “REWARD — Orcs near Wyvern Tor! Those of a mind to face the orc menace should inquire within.” The notice bears the town’s seal and an indecipherable signature.\",\n","              \"Tresendar Manor\": \"A ruined manor. The Redbrands’ base in Phandalin is a dungeon complex under Tresendar Manor. Before the manor was ruined, its cellars served as safe storage for food and water in the event that the estate was attacked, while an adjoining crypt provided a resting place for the deceased members of the Tresendar family. The Redbrands have since expanded the cellars to suit their own purposes, adding slave pens, workshops, and barracks.\"\n","              }\n","\n","# Details of individuals in the town\n","town_people = {\"Toblen Stonehill\": \"Toblen owns a trading post.\",\n","               \"Daran Edermath\": \"Daran is a retired adventurer who lives in a tidy little cottage beside an apple orchard. A fit, silver-haired half-elf well over a hundred years old, Daran is a fighter who served as a marshal and herald for many years in the lands of the Dragon Coast, far to the southeast. Upon retiring, he returned to the Neverwinter region, his original home.\",\n","               \"Linene Graywind\": \"Linene runs a trading post.\",\n","               \"Halia Thornton\": \"Halia is an ambitious and calculating human woman. She is the guildmaster of Phandalin Miner’s Exchange, a trading post where local miners have their valuable finds weighed, measured, and paid out. In her attempts to establish the Miner's Exchange as the closest thing the town has to a governing authority, she acts as more than a simple merchant.\",\n","               \"Qelline Alderleaf\": \"Qelline is a wise female halfling of forty-five, and is a pragmatic farmer who seems to know everything that goes on in town. She is a kind host, and is willing to let the characters stay in her hayloft if they don't want to stay at the Stonehill Inn.\",\n","               \"Sister Garaele\": \"Sister Garaele is an elf cleric of Tymora and a Harper agent.\",\n","               \"Harbin Wester\": \"Harbin is the townmaster of Phandalin. A pompous, old food. Phandalin has no functioning government, but the townsfolk elect someone to serve as townmaster each year. The townmaster serves as a judge in minor disputes and keeps any records that need to be kept.\",\n","               \"Terrill Bloodscar\": \"Terrill is a human ruffian. He wears a grimy scarlet cloak. He is a member of the Redbrand ruffians. He doesn't like adventurers, and wants to rob and kill them.\",\n","               \"Conrad Scarface\": \"Conrad is a human ruffian. He wears a grimy scarlet cloak. He is a member of the Redbrand ruffians. He doesn't like adventurers, and wants to rob and kill them.\",\n","               \"Nellie Starsmith\": \"Nellie is a human ruffian. She wears a grimy scarlet cloak. She is a member of the Redbrand ruffians. She doesn't like adventurers, and wants to rob and kill them.\",\n","               \"Valerie Grinblade\": \"Valerie is a human ruffian. She wears a grimy scarlet cloak. She is a member of the Redbrand ruffians. She doesn't like adventurers, and wants to rob and kill them.\",\n","               }\n","\n","# Populating the world graph with nodes and edges\n","for town_area in town_areas.keys():\n","  world_graph.add_node(town_area)\n","  world_graph.add_edge(town_area, town_area)\n","for town_area in town_areas.keys():\n","  world_graph.add_edge(town_area, \"Phandalin Town Square\")\n","locations = {}\n","for i in town_people.keys():\n","  locations[i] = \"Phandalin Town Square\"\n","\n","# Initializing dictionaries for locations, memories, and plans\n","memories = {}\n","for i in town_people.keys():\n","  memories[i] = []\n","plans = {}\n","for i in town_people.keys():\n","  plans[i] = []\n","\n","# Global time variable\n","global_time = 8"],"metadata":{"id":"8j18ru-dt0sl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to generate area descriptions\n","def generate_description_of_area(x):\n","  text = \"It is \"+str(global_time)+\":00. The location is \"+x+\".\"\n","  people = []\n","  for i in locations.keys():\n","    if locations[i] == x:\n","      people.append(i)"],"metadata":{"id":"3TANlxL42S36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating an empty dictionary for each person in town_people\n","compressed_memories_all = {}\n","for name in town_people.keys():\n","  compressed_memories_all[name] = []"],"metadata":{"id":"zE8sZPfs5HgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loop through each person in town_people to generate prompts and collect plans\n","for name in town_people.keys():\n","    # Constructing a prompt for each individual\n","    prompt = \"You are {}. {} You just woke up in the town of Phandalin and went out to the Town Square. The following people live in the town: {}. What is your goal for today? Be brief, and use at most 20 words and answer from your perspective.\".format(name, town_people[name], ', '.join(list(town_people.keys())) )\n","\n","    # Generating plans using the constructed prompt and storing them\n","    plans[name] = generate(prompt_meta.format(prompt))\n","\n","    # Displaying the name and their corresponding plan\n","    print(name, plans[name])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jckJnxqc69dO","outputId":"36d00298-741e-4c1e-ecf1-48a389444a4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Toblen Stonehill Buying a new book. This will help me improve my research on the trade secrets of the Phandalin Trade Company.\n","Daran Edermath Become more familiar with the town. ### - Welcome to Phandalin.\n","Linene Graywind Linene's goal for today is to increase retail sales at the trading post by offering a special discount to customers who bring in a referral.\n","Halia Thornton Start a conversation with the other players in the Town Square.\n","Qelline Alderleaf The goal for me is to learn more about the town and its inhabitants.\n","Sister Garaele My goal for today is to collect evidence of Harper activities in Phandalin.\n","Harbin Wester Harbin's goal for today is to greet the townsfolk and announce the opening of Phandalin's new cafe. He'll also investigate the strange noises emanating from the town square in search of new customers.\n","Terrill Bloodscar My goal for today is to kill and rob adventurers in the nearby forest.\n","Conrad Scarface I'm going to take a look around the town square and see what I can find out.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Nellie Starsmith Go to the Baron's mansion, kill a hostage, and get myself out of Phandalin alive.\n","Valerie Grinblade My goal today is to terrorize the adventurers in Phandalin.\n"]}]},{"cell_type":"code","source":["# Generating action prompts for each individual in different town areas\n","action_prompts = {}\n","\n","# Iterating through each town area\n","for location in town_areas.keys():\n","    people = []\n","\n","    # Collecting people present in the current location\n","    for i in town_people.keys():\n","        if locations[i] == location:\n","            people.append(i)\n","\n","    # Creating prompts for each person in the location\n","    for name in people:\n","        # Constructing a prompt based on the person's information, plans, and the current location\n","        prompt = \"You are {}. {} You are planning to: {}. You are currently in {} with the following description: {}. It is currently {}:00. The following people are in this area: {}. You can interact with them.\".format(\n","            name, town_people[name], plans[name], location, town_areas[location], str(global_time), ', '.join(people))\n","\n","        # Adding information about people in the area to the prompt\n","        people_description = [i + ': ' + town_people[i] for i in people]\n","        prompt += ' You know the following about people: ' + '. '.join(people_description)\n","\n","        # Adding recent memories to the prompt\n","        memory_text = '. '.join(memories[name][-10:])\n","        prompt += \"What do you do in the next hour? Use at most 10 words to explain.\"\n","\n","        # Storing the generated prompt for the individual\n","        action_prompts[name] = prompt"],"metadata":{"id":"FJ5Dle2t5pAe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generating action results for each person and modifying the tense\n","action_results = {}\n","\n","# Looping through each person in town_people\n","for name in town_people.keys():\n","    # Generating the action result based on the action prompt for the individual\n","    action_results[name] = generate(prompt_meta.format(action_prompts[name]))\n","\n","    # Modifying the tense from present to past\n","    tense_conversion = \"\"\"\n","    Convert the following paragraph to first person past tense:\n","    \"{}\"\n","    \"\"\".format(action_results[name])\n","\n","    # Converting the tense and cleaning the output\n","    action_results[name] = generate(prompt_meta.format(tense_conversion)).replace('\"', '').replace(\"'\", '')\n","\n","    # Displaying the name and the modified action result\n","    print(name, action_results[name])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_B_eAuzsvyjs","outputId":"6697ec37-e9ec-40d3-9243-7224f5e378b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (855 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["Toblen Stonehill I went to the bookstall near Town Square to buy a new book.\n","Daran Edermath I explored the town. Observed the people. Take notes.\n","Linene Graywind Linnet had to remain alert and be prepared for any potential interactions with the ruffians in the next hour.\n","Halia Thornton I wanted to have a chat. I was free any time after 8pm.\n","Qelline Alderleaf I have been eating Toblen, Sister Garaele, Harbin, Halia, Terrill, and Conrad.\n","Sister Garaele I investigated the town square, identified criminals, snooped for evidence, and took pictures for the Harpers.\n","Harbin Wester Unveiled new cafe, investigated noises. Greeted townsfolk, helped find new customers.\n","Terrill Bloodscar I entered the nearby forest and set up camp with the Redbrand ruffians. I stole their loot and waited for them to stumble into the square. I then joined them in their assault on the town, declaring my mission accomplished.\n","Conrad Scarface I took a look around town and saw who else was around.\n","Nellie Starsmith I killed a Redbrand hostage. I went to the Barons mansion. I killed a Redbrand guard. I made it out of Phandalin alive.\n","Valerie Grinblade I engage in various mischief to amuse my gang of ruffians and terrorize the adventurers of Phandalin.\n"]}]},{"cell_type":"markdown","source":["## Collecting the memories Agent's Perceieve"],"metadata":{"id":"rLYWQcuDxfks"}},{"cell_type":"code","source":["# Updating memories for individuals based on their actions in specific locations and current time\n","action_prompts = {}\n","\n","# Iterating through each town area\n","for location in town_areas.keys():\n","    people = []\n","\n","    # Collecting people present in the current location\n","    for i in town_people.keys():\n","        if locations[i] == location:\n","            people.append(i)\n","\n","    # Pairing individuals in the same location\n","    for name in people:\n","        for name_two in people:\n","            # Creating memory entries for each person based on others' actions\n","            memories[name].append('[Time: {}. Person: {}. Memory: {}]\\n'.format(str(global_time),"],"metadata":{"id":"qL0YTTv_3znD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Ranking Agent Memories"],"metadata":{"id":"MmLy4OYssH32"}},{"cell_type":"code","source":["# Importing the regular expression module\n","import re\n","\n","# Function to extract the minimum numerical rating from a string\n","def get_rating(x):\n","    # Extracting all integers from the input string\n","    nums = [int(i) for i in re.findall(r'\\d+', x)]\n","\n","    # Returning the minimum value if at least one number is found\n","    if len(nums) > 0:\n","        return min(nums)\n","    else:\n","        return None\n"],"metadata":{"id":"jaUW3jeqsHV-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating memory ratings for individuals based on their memories and respective care levels\n","memory_ratings = {}\n","\n","# Looping through each person in town_people\n","for name in town_people.keys():\n","    memory_ratings[name] = []\n","\n","    # Iterating through each memory of the person\n","    for i, memory in enumerate(memories[name]):\n","        # Constructing a prompt to rate the importance of the memory\n","        prompt = \"You are {}. Your plans are: {}. You are currently in {}. It is currently {}:00. You observe the following: {}. Give a rating, between 1 and 5, to how much you care about this.\".format(name, plans[name], locations[name], str(global_time), memory)\n","\n","        # Generating a response and extracting the rating\n","        res = generate(prompt_meta.format(prompt))\n","        rating = get_rating(res)\n","\n","        # Setting a maximum number of attempts to extract the rating\n","        max_attempts = 2\n","        current_attempt = 0\n","\n","        # Reattempting to extract the rating if not obtained within the set attempts\n","        while rating is None and current_attempt < max_attempts:\n","            rating = get_rating(res)\n","            current_attempt += 1\n","\n","        # Assigning a default rating if none extracted\n","        if rating is None:\n","            rating = 0\n","\n","        # Appending the memory and its corresponding rating to the memory_ratings dictionary\n","        memory_ratings[name].append((res, rating))\n","\n","    # Displaying the collected memory ratings for each person\n","    print(memory_ratings[name])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jnqRL7-H5Xj8","outputId":"f12a3eff-7f10-4549-bf44-2fb75f9849a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('5 You care very much about this. I have a great passion for researching the Phandalin Trade Company.', 5), ('3 due to the minor details. I care about this because this will help me improve my research on the trade secrets of the Phandalin Trade Company.', 3), ('1 out of 5. It is not necessary for me to care about this as I currently have other priorities.', 1), ('3/5 stars. I care about this as it will help me improve my research on the trade secrets of the Phandalin Trade Company. It will also allow me to spend more time in the square and meet new people. Furthermore, this will give me a chance to practice my conversational skills.', 3), (\"I care about this very much. At a rating of 5, I would rate this observation as a 4 out of 5. Observation is how important Toblen Stonehill's research is and how it can help him complete his latest project.\", 4), ('4/5. This is important information I need to enhance my research.', 4), ('3 out of 5 ratings. I will continue my research on the trade secrets of the Phandalin Trade Company and find new books to improve my research.', 3), ('5 (Must have cared)', 5), ('2 because it does not matter a great deal to me.', 2), ('4/5. This information is highly relevant to my research on the Phandalin Trade Company and its trade secrets.', 4), ('3/5. I care about this because it could lead to useful information about the Phandalin Trade Company.', 3)]\n","[('I care about this very much. It is an important part of my discovery of Phandalin.', 0), ('5 I care about this very much. This experience will be something that I will cherish for a long time.', 5), (\"3.5. I care about Linnet's situation.\", 3), ('3/5. I care about this because it is a way for me to discover the depth of my hometown and meet people.', 3), ('2 (Camera not working)', 2), ('4 because I care about this. I want to become better acquainted with the town and I want to learn more about Sister Garaele.', 4), ('4 I care greatly about the actions of Harbin Wester and am proud to observe the improvements he has made to the town. He dedicated his time in a positive manner and has brought much-needed change to the area.', 4), ('1 I care about this. I want to learn more about Terrill, as well as the events of this past night.', 1), ('2/5. Having made the acquaintance of Conrad Scarface, I am now going to explore the town further.', 2), ('4 (Really helpful) p>', 4), (\"3 because I don't care. I'm just looking for some entertainment and I am so new to Phandalin that I can't really care.\", 3)]\n","[(\"3 (Little) care. However, it might be useful to check in on Toblen just to be sure he's still in Phandalin Town Square and what else around the area is happening.\", 3), (\"4 (Only a small portion of Linene's current thoughts are connected to this)\", 4), ('5. Linene cares greatly about the safety of the trading post and the Phandalin people. She has taken the necessary steps to ensure this.', 5), ('2 because it is not immediately relevant to my job.', 2), ('3/5 I care about this quite a bit since I want to improve my customer acquisition rate at the trading post.', 3), ('3/5. Sister Garaele deserves credit for her excellent investigation, snooping, and taking photos and videos for the Harpers. She deserves to be commended for her bravery.', 3), (\"Linene cares highly about this, as it's important to her progress towards her goal.\", 0), (\"2/5. Although I know it is reminiscent of a past crime, I have no interest in taking part in it to further my goals. I'm more interested in making new connections and developing opportunities with clients.\", 2), (\"3 out of 5 stars. Linene is probably not as concerned about the person as she is the goal she is trying to accomplish. She could give it a 1 out of 5, but it's not really relevant to her goal. She is much more focused on the goal she has set for today in Phandalin Town Square, which is to increase retail sales at the trading post.\", 1), ('I care about this, but only minimally.', 0), ('I rate this observation a 1 out of 5.', 1)]\n","[('I care about this a lot! I would really like to see how Toblen and other players handle it after the game.', 0), ('2 out of 5 stars. I enjoy exploring the town and visiting new places, and I want to talk to the people, so this rating means a lot.', 2), ('3/5. This suggests that I should care about that situation.', 3), ('4 (Adequate)! My intention is to start a conversation at 8pm. I value the time and energy that I gave to this task. Please let me know if you are interested in engaging in this discussion.', 4), ('5 I care about this, since it informs me of the progress made by the other players.', 5), ('4 I care very much about this, it is an important part of my journey. I look forward to discussing it with Sister Garaele in the coming days.', 4), (\"2/5. Although I don't care very much about the other players, I appreciate Harbin's work, and am proud to have met him. It also shows that I should strive to be good neighbours and help out when needed.\", 2), (\"3/5. I care a lot about the story they shared and I'm rooting for them to be successful in their mission.\", 3), ('3/5 I care about this. It could help me start a conversation with other players.', 3), (\"4 This task is important, since it helps to understand the ins and outs of the player's game. It may help to pick on one other player, or to talk to a friend to get a better understanding of what's going on in town. Good luck!\", 4), (\"3/5. I care about Valerie Grinblade's actions as it shows the level of player control and self-interest. She was acting reckless and out of control and it highlights how the game should be approached.\", 3)]\n","[('3 out of 5 stars. While I am not actively looking for information or people to observe, I do care about the people in my environment and am observant. This situation illustrates that I care about my surroundings and am interested in learning more about Phandalin Town Square.', 3), ('5 - I care deeply about learning more about the town and its inhabitants.', 5), (\"3/5. I care about the details of Linnet's situation and what it meant for her to be prepared.\", 3), (\"2 because I don't care about it that much.\", 2), ('3 (Because it has nothing to do with learning more about the town and its inhabitants).', 3), ('4 out of 5. My goal is to learn more about the town and its inhabitants, so I care greatly about this, and I am eager to see what the morning has in store.', 4), ('5 (important) because it helps me learn more about the town and its people.', 5), (\"3 (Not Very Concerned) - I have a goal of learning more about the town and its inhabitants, and while I'm not necessarily concerned about the individual involved, I'm still curious to find out what else Qelline has to say.\", 3), ('3/5 I care about this, as it will help me learn more about the town and its inhabitants.', 3), ('3 out of 5 stars. I am interested in learning more about the story behind Nellie Starsmith and her family.', 3), ('2/5. I care about this as it illustrates the way Valerie Grinblade acts, so I rate it highly.', 2)]\n","[(\"I care about this highly. It is important to make sure the Harper administration does not take advantage of Phandalin's citizens. I will immediately review the situation and do my best to uncover evidence.\", 0), (\"5 (mostly): Sister Garaele is engaged in collecting evidence of Harper's activities in Phandalin.\", 5), (\"2 (Minimum) Because I have already collected evidence from Linnet's home by 8:00 and I'm looking for the exact same scene at 8:00, so I do not care as much about it.\", 0), (\"4/5. I care deeply about making sure Harper's activities are not known to the wider community in Phandalin.\", 4), ('3/5 stars. This helps me out a lot.', 3), ('3/5. This has been a tedious task that I have been struggling with, so I could use less energy and focus on other tasks. However, it is essential that I continue to search for evidence to further my goal and ensure the safety of those in Phandalin. In order to achieve this, I need to be efficient with my resources, so I have adapted the mission to be shorter and more efficient. I am also looking at ways to leverage my network and gain access to more information. I see that the environment is becoming more and more dangerous and that I am needed to collect evidence for the Harpers. Lastly, I need to make connections with members of the town to better gain access to their resources. By the end of the mission,', 3), ('3/5. You are taking care of the evidence you need to complete your goal.', 3), (\"3/5. Seeing this confirms my suspicions that Terrill is hiding evidence of Harper's actions in Phandalin. The fact that he had the time, resources, and courage to attack the town square at such an early hour is more than enough for me to trust him. I'll be back in Phandalin to look for evidence and see if I can bring this person to justice.\", 3), ('4 (Much more important than other tasks).', 4), ('3/5 stars. I have been gathering evidence of Harper’s activities in Phandalin and I have noticed Nellie Starsmith as they stand in the guise of saving it from destruction; I am intrigued by what she is doing.', 3), ('3/5 stars. Since this information is important to my mission, I will use it to further my investigation.', 3)]\n","[('3/5. The townsfolk should know that Harbin is present in the square and will be able to provide assistance if needed. As far as the cafe opening, there is likely going to be some confusion between new customers and existing customers. Therefore, Harbin should ensure that the cafe is prepared for the expected crowd. He should also investigate the strange noises from the town square to determine the cause. Harbin is definitely making progress towards his goal of increasing the number of customers.', 3), ('3 (Not very important) - Harbin is not aware of any potential threats and is focused on accomplishing his mission.', 3), ('4; I care very much about Linnet.', 4), ('3/5. I am not interested in this yet.', 3), (\"3/5. I care about the information provided, but I didn't understand why it is important.\", 3), ('3/5. Harbin cares greatly about investigating the mysterious noises emanating from the town square. He is eager to learn more about the criminals who are at large, and to share his discoveries with the townspeople. With a clear plan of action, he can start the day off on the right foot and increase his chances of success.', 3), ('5 (Top Priority) nooutput>', 5), (\"3/5 stars. I'm aware of the fact that Terrill's memory isn't the best, but he should have a hint as to why, or he could have used the opportunity to gain more attention.\", 3), (\"1 (Don't care) #Acknowledgement #Planning #Recording\", 1), ('3 (Not important) since your other goal is to greet the townsfolk and announce the opening of the new cafe within the next few hours.', 3), (\"3/5, because Harbin does not have time to concern himself with the current situation. He'll be able to concentrate on the tasks at hand.\", 3)]\n","[('3 (Not important) and will continue on with my plans.', 3), ('3/5 because I care very little about this.', 3), (\"I care about this quite a bit - I understand how critical it is for Linene to be prepared for any potential interactions with the ruffians, and I'm sure they appreciate my interest.\", 0), (\"3 out of 5, as this won't have any effect on my plan.\", 3), (\"3/10. I have no interest in Qelline's memory, so I rate it unimportant. Your goal of the day should be to kill and rob adventurers. I will not have much time for this!\", 3), (\"5 (Caring) - Sister Garaele's work for the Harpers is a commendable effort and she should be commended for her dedication to her work.\", 5), ('3 out of 5 because it is irrelevant to my plan for today. I care more about the killing and robbery in the forest.', 3), (\"3/5. Terrill's memory is incredibly detailed, and it gives a clear insight into what motivated him to kill and rob adventurers. It makes it easier for you compare his performance to the objective of the day and to gauge his willingness to keep going.\", 3), ('4 (Not necessary) I am not concerned by this as it is not relevant to my current goal.', 4), ('5 (I care a lot about this)', 5), (\"3 (Don't care) as I simply want to make my mission in the nearby forest a success and make my time in Phandalin go by as fast as possible.\", 3)]\n","[('4 The fact that I can purchase a new book is of great importance to me.', 4), ('5: I care about this a lot.', 5), (\"3/5 because I care about how Linnet is feeling because it's important for her to be aware of potential dangers. However, I am also looking for information, so I can use this information to further my own investigation.\", 3), ('4 out of 5, because I care about this.', 4), ('2 (too little care) #Conrad Scarface', 2), ('4. You care deeply about helping the people of Phandalin and investigating the town square to find out what happened before and what sinister consequences may arise.', 4), ('4/5. I care highly about Harbin Wester and what she has done to contribute to the community.', 4), (\"3 Rating: None. I'm sure Terrill did it out of convenience.\", 3), ('4 (Not very important) #Remember this for later.', 4), ('3/5. This experience is a milestone to be celebrated and remembered. It is an opportunity to explore and grow deeper into my relationships with those around me.', 3), ('I care about this, but only to a certain extent.', 0)]\n","[(\"3/5. This is less important than the more important goals I'm trying to accomplish.\", 3), ('1 out of 5 stars. Not particularly interesting, and it would be better if the other objectives were complete.', 1), ('4 out of 5. This gives me a sense of how much Linnet is important to me, and I need to keep her as part of my plans.', 4), ('4/5 rating. This action is very important in achieving your goals and makes it that much more likely that you will make it out of Phandalin alive.', 4), ('3/5 because there is no way it could help me complete my plan.', 3), ('3/5. This reminds me of the importance of staying focused and taking action. I must make sure to avoid distractions and stay ahead of the game.', 3), (\"I care about this, but it is not important compared to my main goals. I need to focus on getting to the Baron's mansion and killing the hostage that is guarding it. So, this memory should not be a priority for me.\", 0), (\"3 out of 5 stars. I don't care much for this moment since it won't help me achieve my goals.\", 3), ('2/5. Conrad Scarface is a friendly local who I am more than excited to meet as I venture into Phandalin Town Square today. I am looking forward to having him be my guide as I venture further into the town. We get to talk, catch up on the latest news, and get all the facts in town before I have to make a quick exit.', 2), ('5 I care about this because it is a representation of how important my survival is. This is my memory of events from earlier that day.', 5), ('3 (Not very important). Nellie needs to worry more about the plan that she needs to execute.', 3)]\n","[(\"3/5. I care about this because it's a plot point for completing my task of terrorizing the adventurers.\", 3), ('3/5. I care about this very little since I am primarily focused on my own goal, but it is important to know what else I should do this morning.', 3), ('3 (Not very important to me, since my goal is to terrorize the adventurers).', 3), ('2 (Not important since I am still pursuing my goal to terrorize the adventurers).', 2), (\"3/5. Qelline Alderleaf's eating habits provide an excellent opportunity for me to gain further insight into her personal struggles, which can provide useful motivation for me to further terrorize the adventurers in Phandalin.\", 3), ('4 with no care. It is a waste of time and energy for me.', 4), (\"1 out of 5. I don't care at all about Harbin Wester's actions.\", 1), ('5 (It is a major concern that Valerie Grinblade is taking the initiative in this situation.)', 5), ('4 because it is very useful information for my goal.', 4), ('4 - I care about this a lot.', 4), ('3/5. I was able to accomplish my goal of terrorizing the adventurers in Phandalin and I enjoyed the experience immensely.', 3)]\n"]}]},{"cell_type":"markdown","source":["# Compressing Memories"],"metadata":{"id":"5dtUMeBcysQO"}},{"cell_type":"code","source":["# Setting a memory limit and compressing memories for each individual\n","MEMORY_LIMIT = 10\n","compressed_memories = {}\n","\n","# Looping through each person in town_people\n","for name in town_people.keys():\n","    # Sorting memory ratings for the person based on importance\n","    memories_sorted = sorted(memory_ratings[name], key=lambda x: x[1])[::-1]\n","\n","    # Selecting relevant memories up to the memory limit\n","    relevant_memories = memories_sorted[:MEMORY_LIMIT]\n","\n","    # Concatenating relevant memories into a string for compression\n","    memory_string_to_compress = '.'.join([a[0] for a in relevant_memories])\n","\n","    # Constructing a prompt to summarize the compressed memories\n","    prompt = \"You are {}. Your plans are: {}. You are currently in {}. It is currently {}:00. You observe the following: {}. Summarize these memories in one sentence.\".format(\n","        name, plans[name], locations[name], str(global_time), memory_string_to_compress)\n","\n","    # Generating a response summarizing the compressed memories\n","    res = generate(prompt_meta.format(prompt))\n","\n","    # Creating a compressed memory entry for the individual\n","    compressed_memories[name] = '[Recollection at Time {}:00: {}]'.format(str(global_time), res)\n","\n","    # Appending the compressed memory to the overall memories for the individual\n","    compressed_memories_all[name].append(compressed_memories[name])"],"metadata":{"id":"31iQUQx3xryF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gathering place ratings for individuals and updating their locations\n","place_ratings = {}\n","\n","# Looping through each person in town_people\n","for name in town_people.keys():\n","    place_ratings[name] = []\n","\n","    # Gathering ratings for each town area\n","    for area in town_areas.keys():\n","        # Constructing a prompt to rate the likelihood of being in a specific area\n","        prompt = \"You are {}. Your plans are: {}. You are currently in {}. It is currently {}:00. You have the following memories: {}. Give a rating, between 1 and 5, to how likely you are likely to be at {} the next hour.\".format(\n","            name, plans[name], locations[name], str(global_time), compressed_memories[name], area)\n","\n","        # Generating a response and extracting the rating\n","        res = generate(prompt_meta.format(prompt))\n","        rating = get_rating(res)\n","\n","        # Setting a maximum number of attempts to extract the rating\n","        max_attempts = 2\n","        current_attempt = 0\n","\n","        # Reattempting to extract the rating if not obtained within the set attempts\n","        while rating is None and current_attempt < max_attempts:\n","            rating = get_rating(res)\n","            current_attempt += 1\n","\n","        # Assigning a default rating if none extracted\n","        if rating is None:\n","            rating = 0\n","\n","        # Appending the area, its rating, and the response to place_ratings dictionary\n","        place_ratings[name].append((area, rating, res))\n","\n","    # Sorting place ratings based on likelihood and updating the current location\n","    place_ratings_sorted = sorted(place_ratings[name], key=lambda x: x[1])[::-1]\n","\n","    # Checking if the highest rated area is different from the current location\n","    if place_ratings_sorted[0][0] != locations[name]:\n","        # Creating a new recollection of moving to a different location\n","        new_recollection = '[Recollection at Time {}:00: {}]'.format(str(global_time), 'I then moved to {}.'.format(place_ratings_sorted[0][0]))\n","\n","        # Appending the new recollection to the overall memories for the individual\n","        compressed_memories_all[name].append(new_recollection)\n","\n","    # Updating the current location to the highest rated area\n","    locations[name] = place_ratings_sorted[0][0]"],"metadata":{"id":"crD2ogLjzm6c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Final Output"],"metadata":{"id":"0rPlJg13F1rY"}},{"cell_type":"code","source":["# Function header and comments for generate_action_prompts function\n","def generate_action_prompts(global_time, town_people, locations, plans, town_areas, compressed_memories_all, memories):\n","    \"\"\"\n","    Generates action prompts for individuals based on their current location, plans, memories, and interactions.\n","\n","    Args:\n","    - global_time (int): The current time in the simulation.\n","    - town_people (dict): Dictionary containing information about individuals in the town.\n","    - locations (dict): Dictionary mapping individuals to their current locations.\n","    - plans (dict): Dictionary containing the plans of individuals.\n","    - town_areas (dict): Dictionary containing information about different areas in the town.\n","    - compressed_memories_all (dict): Dictionary containing compressed memories of individuals.\n","    - memories (dict): Dictionary containing memories of individuals.\n","\n","    Returns:\n","    - action_prompts (dict): Dictionary containing action prompts for each individual.\n","    \"\"\"\n","    action_prompts = {}\n","\n","    # Iterating through each location in town_areas\n","    for location in town_areas.keys():\n","        # Gather people in the current location\n","        people = [i for i in town_people.keys() if locations[i] == location]\n","\n","        # Iterating through each person in the current location\n","        for name in people:\n","            # Construct action prompt for each person\n","            prompt = f\"You are {name}. Your plans are: {plans[name]}. You are currently in {location} with the following description: {town_areas[location]}. Your memories are: {'\\n'.join(compressed_memories_all[name][-5:])}. It is currently {global_time}:00. The following people are in this area: {', '.join(people)}. You can interact with them.\"\n","            people_description = [f\"{i}: {town_people[i]}\" for i in people]\n","            prompt += f\" You know the following about people: {'.'.join(people_description)}\"\n","            memory_text = '. '.join(memories[name][-10:])\n","            prompt += \"What do you do in the next hour? Use at most 10 words to explain.\"\n","\n","            # Storing the prompt for the individual\n","            action_prompts[name] = prompt\n","\n","    return action_prompts\n","\n","\n","# Function header and comments for execute_actions function\n","def execute_actions(action_prompts, prompt_meta):\n","    \"\"\"\n","    Executes actions based on the action prompts generated for individuals.\n","\n","    Args:\n","    - action_prompts (dict): Dictionary containing action prompts for each individual.\n","    - prompt_meta (str): The template for generating prompts.\n","\n","    Returns:\n","    - action_results (dict): Dictionary containing executed actions for each individual.\n","    - action_emojis (dict): Dictionary containing emoji representations of actions for each individual.\n","    \"\"\"\n","    action_results = {}\n","    action_emojis = {}\n","\n","    # Iterating through each action prompt for individuals\n","    for name, prompt in action_prompts.items():\n","        # Generate action result based on the prompt\n","        action_results[name] = generate(prompt_meta.format(prompt))\n","\n","        # Cleaning the action into past tense\n","        past_tense_prompt = f\"Convert the following paragraph to first person past tense: \\\"{action_results[name]}\\\"\"\n","        action_results[name] = generate(prompt_meta.format(past_tense_prompt)).replace('\"', '').replace(\"'\", '')\n","\n","        # Print the action details\n","        print(name, locations[name], global_time, action_results[name])\n","\n","        # Generating emoji representations of actions\n","        emoji_prompt = f\"Convert the following paragraph to a tuple (Action, Object): \\\"{action_results[name]}\\\"\"\n","        action_emojis[name] = generate(prompt_meta.format(emoji_prompt)).replace('\"', '').replace(\"'\", '')\n","        print('    - Emoji Representation:', name, locations[name], global_time, action_emojis[name])\n","\n","    return action_results, action_emojis\n","\n","# Function header and comments for update_memories_ratings function\n","def update_memories_ratings(global_time, town_people, locations, plans, compressed_memories_all, memories, get_rating, prompt_meta):\n","    \"\"\"\n","    Updates memory ratings based on the observations and prompts provided to individuals.\n","\n","    Args:\n","    - global_time (int): The current time in the simulation.\n","    - town_people (dict): Dictionary containing information about individuals in the town.\n","    - locations (dict): Dictionary mapping individuals to their current locations.\n","    - plans (dict): Dictionary containing the plans of individuals.\n","    - compressed_memories_all (dict): Dictionary containing compressed memories of individuals.\n","    - memories (dict): Dictionary containing memories of individuals.\n","    - get_rating (function): Function to extract ratings from responses.\n","    - prompt_meta (str): The template for generating prompts.\n","\n","    Returns:\n","    - memory_ratings (dict): Dictionary containing updated memory ratings for each individual.\n","    \"\"\"\n","    memory_ratings = {}\n","\n","    # Iterating through each person in town_people\n","    for name in town_people.keys():\n","        memory_ratings[name] = []\n","\n","        # Iterating through memories of the person\n","        for i, memory in enumerate(memories[name]):\n","            # Construct prompt for memory rating\n","            prompt = f\"You are {name}. Your plans are: {plans[name]}. Your memories are: {'\\n'.join(compressed_memories_all[name][-5:])}. You are currently in {locations[name]}. It is currently {global_time}:00. You observe the following: {memory}. Give a rating, between 1 and 5, to how much you care about this.\"\n","\n","            # Generating response and extract rating\n","            res = generate(prompt_meta.format(prompt))\n","            rating = get_rating(res)\n","            max_attempts = 2\n","            current_attempt = 0\n","\n","            # Try extracting rating with a limit on attempts\n","            while rating is None and current_attempt < max_attempts:\n","                rating = get_rating(res)\n","                current_attempt += 1\n","\n","            if rating is None:\n","                rating = 0\n","\n","            # Appending response and rating to memory_ratings\n","            memory_ratings[name].append((res, rating))\n","\n","    return memory_ratings\n","\n","\n","# Function header and comments for compress_memories function\n","def compress_memories(global_time, town_people, MEMORY_LIMIT, memory_ratings, compressed_memories_all, plans, locations):\n","    \"\"\"\n","    Compresses memories based on ratings and generates a summary for individuals.\n","\n","    Args:\n","    - global_time (int): The current time in the simulation.\n","    - town_people (dict): Dictionary containing information about individuals in the town.\n","    - MEMORY_LIMIT (int): Maximum number of memories to retain for compression.\n","    - memory_ratings (dict): Dictionary containing memory ratings for individuals.\n","    - compressed_memories_all (dict): Dictionary containing compressed memories of individuals.\n","    - plans (dict): Dictionary containing the plans of individuals.\n","    - locations (dict): Dictionary mapping individuals to their current locations.\n","\n","    Returns:\n","    - compressed_memories (dict): Dictionary containing compressed memories for each individual.\n","    - compressed_memories_all (dict): Updated dictionary containing all compressed memories for each individual.\n","    \"\"\"\n","    compressed_memories = {}\n","\n","    # Iterating through each person in town_people\n","    for name in town_people.keys():\n","        memories_sorted = sorted(memory_ratings[name], key=lambda x: x[1])[::-1]\n","        relevant_memories = memories_sorted[:MEMORY_LIMIT]\n","        memory_string_to_compress = '.'.join([a[0] for a in relevant_memories])\n","\n","        # Constructing prompt for memory compression\n","        prompt = f\"You are {name}. Your plans are: {plans[name]}. You are currently in {locations[name]}. It is currently {global_time}:00. You observe the following: {memory_string_to_compress}. Summarize these memories in one sentence.\"\n","\n","        # Generating compressed memory\n","        res = generate(prompt_meta.format(prompt))\n","        compressed_memories[name] = f'[Recollection at Time {global_time}:00: {res}]'\n","\n","        # Appending compressed memory to compressed_memories_all\n","        compressed_memories_all[name].append(compressed_memories[name])\n","\n","    return compressed_memories, compressed_memories_all\n","\n","# Function header and comments for update_location_ratings function\n","def update_location_ratings(global_time, town_people, locations, town_areas, compressed_memories, compressed_memories_all, place_ratings, get_rating, prompt_meta):\n","    \"\"\"\n","    Updates location ratings based on observations and prompts provided to individuals.\n","\n","    Args:\n","    - global_time (int): The current time in the simulation.\n","    - town_people (dict): Dictionary containing information about individuals in the town.\n","    - locations (dict): Dictionary mapping individuals to their current locations.\n","    - town_areas (dict): Dictionary containing descriptions of different town areas.\n","    - compressed_memories (dict): Dictionary containing compressed memories of individuals.\n","    - compressed_memories_all (dict): Dictionary containing all compressed memories for each individual.\n","    - place_ratings (dict): Dictionary containing location ratings for individuals.\n","    - get_rating (function): Function to extract ratings from responses.\n","    - prompt_meta (str): The template for generating prompts.\n","\n","    Returns:\n","    - locations (dict): Updated dictionary mapping individuals to their current locations.\n","    - place_ratings (dict): Updated dictionary containing location ratings for individuals.\n","    - compressed_memories_all (dict): Updated dictionary containing all compressed memories for each individual.\n","    \"\"\"\n","    for name in town_people.keys():\n","        place_ratings[name] = []\n","\n","        # Iterating through town_areas for location ratings\n","        for area in town_areas.keys():\n","            prompt = f\"You are {name}. Your plans are: {plans[name]}. You are currently in {locations[name]}. It is currently {global_time}:00. You have the following memories: {compressed_memories[name]}. Give a rating, between 1 and 5, to how likely you are likely to be at {area} the next hour.\"\n","            res = generate(prompt_meta.format(prompt))\n","            rating = get_rating(res)\n","            max_attempts = 2\n","            current_attempt = 0\n","\n","            # Try extracting rating with a limit on attempts\n","            while rating is None and current_attempt < max_attempts:\n","                rating = get_rating(res)\n","                current_attempt += 1\n","\n","            if rating is None:\n","                rating = 0\n","\n","            # Appending response, rating, and area to place_ratings\n","            place_ratings[name].append((area, rating, res))\n","\n","        # Sorting place_ratings and update locations\n","        place_ratings_sorted = sorted(place_ratings[name], key=lambda x: x[1])[::-1]\n","        if place_ratings_sorted[0][0] != locations[name]:\n","            new_recollection = f'[Recollection at Time {global_time}:00: I then moved to {place_ratings_sorted[0][0]}.]'\n","            compressed_memories_all[name].append(new_recollection)\n","        locations[name] = place_ratings_sorted[0][0]\n","\n","    return locations, place_ratings, compressed_memories_all\n","\n","\n","# Initializing necessary variables and dictionaries here\n","\n","for repeats in range(5):\n","    global_time += 1\n","\n","    # Generating action prompts\n","    action_prompts = generate_action_prompts(global_time, town_people, locations, plans, town_areas, compressed_memories_all, memories)\n","\n","    # Executing actions\n","    action_results, action_emojis = execute_actions(action_prompts, prompt_meta)\n","\n","    # Updating memory ratings\n","    memory_ratings = update_memories_ratings(global_time, town_people, locations, plans, compressed_memories_all, memories, get_rating, prompt_meta)\n","\n","    # Compressing memories\n","    compressed_memories, compressed_memories_all = compress_memories(global_time, town_people, MEMORY_LIMIT, memory_ratings, compressed_memories_all, plans, locations)\n","\n","    # Updating location ratings\n","    update_location_ratings(global_time, town_people, locations, town_areas, compressed_memories, compressed_memories_all, place_ratings, get_rating, prompt_meta)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":880},"id":"h_ic4rpaF5Ui","outputId":"1cfdcb16-3f57-44b7-e712-e538119c8b71"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Toblen Stonehill Edermath Orchard 16 I explored the area to find a new book that will help me improve my research on the trade secrets of the Phandalin Trade Company. I may ask Terrill Bloodscar for help, as Terrill is a ruffian I know.\n","Daran Edermath The Sleeping Giant 16 Daran Edermath talked to Halia Thornton and Sister Garaele.\n","Linene Graywind Phandalin Town Square 16 Linene created a marketing campaign to increase retail sales at the trading post.\n","Halia Thornton The Sleeping Giant 16 I got the conversation started. I discussed the upcoming events and had a fun time.\n","Qelline Alderleaf Tresendar Manor 16 Quilline went to the Shrine of Luck, focused on Daran Edermath, visited his family. She was interested in learning about their familys history and culture, including the traditions of their faith and other aspects of the history or magic in Phandalin. After visiting his family, Quilline moved to Tresendar Manor, with the goal of talking to Valerie and learning about the locals. There she hoped to uncover the adventures of the Redbrands and uncover the mysteries that the Redbrands kept open.\n","Sister Garaele The Sleeping Giant 16 I found evidence of Harper activity in Phandalin.\n","Harbin Wester Shrine of Luck 16 I went to the shrine for a meal and talked to the people.\n","Terrill Bloodscar Edermath Orchard 16 I searched the nearby forest for adventurers.\n","Conrad Scarface Phandalin Town Square 16 I took a look around the town square and see what I could find out.\n","Nellie Starsmith Alderleaf Farm 16 I collected supplies, went to Barons mansion, killed a hostage, and escaped Phandalin.\n","Valerie Grinblade Tresendar Manor 16 I waited for the adventurers to arrive, then I raided and killed them.\n","    - Emoji Representation: Explored [action], Found [object]; May ask (object) [Terrill Bloodscar, ruffian] for help;\n","    - Emoji Representation: Daran Edermath talk TO Halia Thornton, Sister Garaele\n","    - Emoji Representation: Linnete: Created a marketing campaign to increase retail sales at the trading post.\n","    - Emoji Representation: Acted on [I got the conversation started, Discussed the upcoming events, Had a fun time]\n","    - Emoji Representation: Shrine of Luck, Daran Edermath, Family (Inquired about his familys history and culture, including traditions of faith, and other aspects of history/magic in Phandalin) Tresendar Manor, Valerie, locals, uncover Redbrands adventures, uncover mysteries\n","    - Emoji Representation: Found evidence Harper activity Phandalin.\n","    - Emoji Representation: I went to the shrine -> had a meal -> talked to the people.\n","    - Emoji Representation: Searched (Action), Object (Adventurers)\n","    - Emoji Representation: I: Taking a look around the town square Object: What I could find out\n","    - Emoji Representation: Action: Collect supplies, Go to Barons mansion, Kill a hostage, Escape Phandalin.\n","    - Emoji Representation: Action: Waited, Object: Adventurers\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-efddc0c89f61>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"You are {}. Your plans are: {}. Your memories are: {}. You are currently in {}. It is currently {}:00. You observe the following: {}. Give a rating, between 1 and 5, to how much you care about this.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompressed_memories_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m       \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m       \u001b[0mrating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0mmax_attempts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-b1f82efcaf62>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m### Response:'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generated_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m'### Response:'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \"\"\"\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         if (\n\u001b[1;32m    167\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             )\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"min_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1486\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2524\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2525\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1717\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 )\n\u001b[1;32m   1085\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1087\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    694\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    598\u001b[0m     ):\n\u001b[1;32m    599\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["### As main"],"metadata":{"id":"h_vHDPFm4ld5"}},{"cell_type":"code","source":["for repeats in range(5):\n","  global_time += 1\n","  action_prompts = {}\n","  for location in town_areas.keys():\n","    people = []\n","    for i in town_people.keys():\n","      if locations[i] == location:\n","        people.append(i)\n","\n","    for name in people:\n","      prompt = \"You are {}. Your plans are: {}. You are currently in {} with the following description: {}. Your memories are: {}. It is currently {}:00. The following people are in this area: {}. You can interact with them.\".format(name, plans[name], location, town_areas[location], '\\n'.join(compressed_memories_all[name][-5:]), str(global_time), ', '.join(people))\n","      people_description = []\n","      for i in people:\n","        people_description.append(i+': '+town_people[i])\n","      prompt += ' You know the following about people: ' + '. '.join(people_description)\n","      memory_text = '. '.join(memories[name][-10:])\n","      prompt += \"What do you do in the next hour? Use at most 10 words to explain.\"\n","      action_prompts[name] = prompt\n","  action_results = {}\n","  for name in town_people.keys():\n","    action_results[name] = generate(prompt_meta.format(action_prompts[name]))\n","    # Now clean the action\n","    prompt = \"\"\"\n","    Convert the following paragraph to first person past tense:\n","    \"{}\"\n","    \"\"\".format(action_results[name])\n","    action_results[name] = generate(prompt_meta.format(prompt)).replace('\"', '').replace(\"'\", '')\n","    print(name, locations[name], global_time, action_results[name])\n","  action_emojis = {}\n","  for name in town_people.keys():\n","    prompt = \"\"\"\n","    Convert the following paragraph to a tuple (Action, Object):\n","    \"{}\"\n","    \"\"\".format(action_results[name])\n","    action_emojis[name] = generate(prompt_meta.format(prompt)).replace('\"', '').replace(\"'\", '')\n","    print('    - Emoji Representation:', name, locations[name], global_time, action_emojis[name])\n","  action_prompts = {}\n","  for location in town_areas.keys():\n","    people = []\n","    for i in town_people.keys():\n","      if locations[i] == location:\n","        people.append(i)\n","\n","    for name in people:\n","      for name_two in people:\n","        memories[name].append('[Time: {}. Person: {}. Memory: {}]\\n'.format(str(global_time), name_two, action_results[name_two]))\n","\n","  memory_ratings = {}\n","  for name in town_people.keys():\n","    memory_ratings[name] = []\n","    for i, memory in enumerate(memories[name]):\n","      prompt = \"You are {}. Your plans are: {}. Your memories are: {}. You are currently in {}. It is currently {}:00. You observe the following: {}. Give a rating, between 1 and 5, to how much you care about this.\".format(name, plans[name], '\\n'.join(compressed_memories_all[name][-5:]), locations[name], str(global_time), memory)\n","      res = generate(prompt_meta.format(prompt))\n","      rating = get_rating(res)\n","      max_attempts = 2\n","      current_attempt = 0\n","      while rating is None and current_attempt<max_attempts:\n","        rating = get_rating(res)\n","        current_attempt += 1\n","      if rating is None:\n","        rating = 0\n","      memory_ratings[name].append((res, rating))\n","\n","  compressed_memories = {}\n","  for name in town_people.keys():\n","    memories_sorted = sorted(\n","          memory_ratings[name],\n","          key=lambda x: x[1]\n","      )[::-1]\n","    relevant_memories = memories_sorted[:MEMORY_LIMIT]\n","    memory_string_to_compress = '.'.join([a[0] for a in relevant_memories])\n","    prompt = \"You are {}. Your plans are: {}. You are currently in {}. It is currently {}:00. You observe the following: {}. Summarize these memories in one sentence.\".format(name, plans[name], locations[name], str(global_time), memory_string_to_compress)\n","    res = generate(prompt_meta.format(prompt))\n","    compressed_memories[name] = '[Recollection at Time {}:00: {}]'.format(str(global_time), res)\n","    compressed_memories_all[name].append(compressed_memories[name])\n","\n","  place_ratings = {}\n","\n","  for name in town_people.keys():\n","    place_ratings[name] = []\n","    for area in town_areas.keys():\n","      prompt = \"You are {}. Your plans are: {}. You are currently in {}. It is currently {}:00. You have the following memories: {}. Give a rating, between 1 and 5, to how likely you are likely to be at {} the next hour.\".format(name, plans[name], locations[name], str(global_time), compressed_memories[name], area)\n","      res = generate(prompt_meta.format(prompt))\n","      rating = get_rating(res)\n","      max_attempts = 2\n","      current_attempt = 0\n","      while rating is None and current_attempt<max_attempts:\n","        rating = get_rating(res)\n","        current_attempt += 1\n","      if rating is None:\n","        rating = 0\n","      place_ratings[name].append((area, rating, res))\n","    place_ratings_sorted = sorted(\n","        place_ratings[name],\n","        key=lambda x: x[1] )[::-1]\n","    if place_ratings_sorted[0][0] != locations[name]:\n","      new_recollection = '[Recollection at Time {}:00: {}]'.format(str(global_time), 'I then moved to {}.'.format(place_ratings_sorted[0][0]))\n","      compressed_memories_all[name].append(new_recollection)\n","    locations[name] = place_ratings_sorted[0][0]"],"metadata":{"id":"1BB_8qwV4m-3"},"execution_count":null,"outputs":[]}]}